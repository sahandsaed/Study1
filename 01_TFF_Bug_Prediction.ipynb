{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Federated (TFF) Bug Prediction Experiment\n",
    "\n",
    "## Study: FL Framework Comparison - TFF vs Flower\n",
    "\n",
    "This notebook evaluates TFF for bug prediction with:\n",
    "- **Communication Cost**: Tracking bytes sent/received per round\n",
    "- **Security Analysis**: Model poisoning (gradient +100) and Data poisoning (label flipping)\n",
    "- **Metrics**: F1 Score, Accuracy, Precision, Recall, Loss (before/after attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow==2.14.0 tensorflow-federated==0.64.0 scikit-learn -q\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"TensorFlow Federated version: {tff.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload dataset_pairs_1_.json\")\n",
    "uploaded = files.upload()\n",
    "dataset_path = list(uploaded.keys())[0]\n",
    "print(f\"\\nUploaded: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTokenizer:\n",
    "    \"\"\"Simple tokenizer for Python code.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=5000, max_length=200):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}\n",
    "        self.word_counts = {}\n",
    "    \n",
    "    def fit(self, codes: List[str]):\n",
    "        for code in codes:\n",
    "            tokens = self._tokenize(code)\n",
    "            for token in tokens:\n",
    "                self.word_counts[token] = self.word_counts.get(token, 0) + 1\n",
    "        \n",
    "        sorted_words = sorted(self.word_counts.items(), key=lambda x: -x[1])\n",
    "        for word, _ in sorted_words[:self.max_vocab_size - len(self.vocab)]:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def _tokenize(self, code: str) -> List[str]:\n",
    "        import re\n",
    "        tokens = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*|[0-9]+|[^\\s\\w]', code)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, code: str) -> np.ndarray:\n",
    "        tokens = self._tokenize(code)\n",
    "        ids = [self.vocab.get(t, 1) for t in tokens[:self.max_length]]\n",
    "        padded = ids + [0] * (self.max_length - len(ids))\n",
    "        return np.array(padded[:self.max_length], dtype=np.int32)\n",
    "\n",
    "\n",
    "class CodeFeatureExtractor:\n",
    "    \"\"\"Extract handcrafted features from code.\"\"\"\n",
    "    \n",
    "    def extract(self, code: str) -> np.ndarray:\n",
    "        features = [\n",
    "            len(code),\n",
    "            code.count('\\n'),\n",
    "            code.count('def '),\n",
    "            code.count('class '),\n",
    "            code.count('if '),\n",
    "            code.count('for '),\n",
    "            code.count('while '),\n",
    "            code.count('try:'),\n",
    "            code.count('except'),\n",
    "            code.count('return'),\n",
    "            code.count('import'),\n",
    "            code.count('='),\n",
    "            code.count('+'),\n",
    "            code.count('-'),\n",
    "            code.count('*'),\n",
    "            code.count('/'),\n",
    "            code.count('['),\n",
    "            code.count('('),\n",
    "            len(code.split()),\n",
    "            code.count('#')\n",
    "        ]\n",
    "        return np.array(features, dtype=np.float32)\n",
    "\n",
    "\n",
    "class BugPredictionDataProcessor:\n",
    "    \"\"\"Process bug prediction dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: CodeTokenizer, feature_extractor: CodeFeatureExtractor):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "    \n",
    "    def load_data(self, json_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        all_codes = []\n",
    "        for pair in data:\n",
    "            all_codes.append(pair['buggy_code'])\n",
    "            all_codes.append(pair['fixed_code'])\n",
    "        \n",
    "        self.tokenizer.fit(all_codes)\n",
    "        \n",
    "        tokens_list = []\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for pair in data:\n",
    "            # Buggy code (label=1)\n",
    "            tokens_list.append(self.tokenizer.encode(pair['buggy_code']))\n",
    "            features_list.append(self.feature_extractor.extract(pair['buggy_code']))\n",
    "            labels_list.append(1)\n",
    "            \n",
    "            # Fixed code (label=0)\n",
    "            tokens_list.append(self.tokenizer.encode(pair['fixed_code']))\n",
    "            features_list.append(self.feature_extractor.extract(pair['fixed_code']))\n",
    "            labels_list.append(0)\n",
    "        \n",
    "        return np.array(tokens_list), np.array(features_list), np.array(labels_list)\n",
    "    \n",
    "    def partition_iid(self, tokens, features, labels, num_clients):\n",
    "        \"\"\"IID partitioning.\"\"\"\n",
    "        indices = np.random.permutation(len(labels))\n",
    "        splits = np.array_split(indices, num_clients)\n",
    "        \n",
    "        client_data = []\n",
    "        for split in splits:\n",
    "            client_data.append({\n",
    "                'tokens': tokens[split],\n",
    "                'features': features[split],\n",
    "                'labels': labels[split]\n",
    "            })\n",
    "        return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "tokenizer = CodeTokenizer(max_vocab_size=5000, max_length=200)\n",
    "feature_extractor = CodeFeatureExtractor()\n",
    "processor = BugPredictionDataProcessor(tokenizer, feature_extractor)\n",
    "\n",
    "tokens, features, labels = processor.load_data(dataset_path)\n",
    "print(f\"Total samples: {len(labels)}\")\n",
    "print(f\"Buggy (1): {sum(labels)}, Non-buggy (0): {len(labels) - sum(labels)}\")\n",
    "print(f\"Token shape: {tokens.shape}, Features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bug_prediction_model(vocab_size=5000, embedding_dim=64, max_length=200, num_features=20):\n",
    "    \"\"\"Create a simple bug prediction model.\"\"\"\n",
    "    # Token input\n",
    "    token_input = tf.keras.layers.Input(shape=(max_length,), name='tokens')\n",
    "    embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)(token_input)\n",
    "    lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(embedding)\n",
    "    \n",
    "    # Feature input\n",
    "    feature_input = tf.keras.layers.Input(shape=(num_features,), name='features')\n",
    "    feature_dense = tf.keras.layers.Dense(32, activation='relu')(feature_input)\n",
    "    \n",
    "    # Combine\n",
    "    combined = tf.keras.layers.Concatenate()([lstm, feature_dense])\n",
    "    dense = tf.keras.layers.Dense(64, activation='relu')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(dense)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[token_input, feature_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Test model creation\n",
    "test_model = create_bug_prediction_model(vocab_size=len(tokenizer.vocab))\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics and Experiment Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClassificationMetrics:\n",
    "    \"\"\"Store classification metrics.\"\"\"\n",
    "    accuracy: float = 0.0\n",
    "    precision: float = 0.0\n",
    "    recall: float = 0.0\n",
    "    f1_score: float = 0.0\n",
    "    loss: float = 0.0\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Store experiment results.\"\"\"\n",
    "    experiment_name: str\n",
    "    metrics: ClassificationMetrics\n",
    "    training_time: float = 0.0\n",
    "    communication_bytes: float = 0.0\n",
    "    round_metrics: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'experiment_name': self.experiment_name,\n",
    "            'metrics': self.metrics.to_dict(),\n",
    "            'training_time': self.training_time,\n",
    "            'communication_bytes': self.communication_bytes,\n",
    "            'round_metrics': self.round_metrics\n",
    "        }\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, loss=0.0) -> ClassificationMetrics:\n",
    "    \"\"\"Calculate all classification metrics.\"\"\"\n",
    "    return ClassificationMetrics(\n",
    "        accuracy=accuracy_score(y_true, y_pred),\n",
    "        precision=precision_score(y_true, y_pred, zero_division=0),\n",
    "        recall=recall_score(y_true, y_pred, zero_division=0),\n",
    "        f1_score=f1_score(y_true, y_pred, zero_division=0),\n",
    "        loss=loss\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_model_size(model) -> int:\n",
    "    \"\"\"Calculate model size in bytes.\"\"\"\n",
    "    total_bytes = 0\n",
    "    for weight in model.get_weights():\n",
    "        total_bytes += weight.nbytes\n",
    "    return total_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TFF Federated Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(client_data, batch_size=32):\n",
    "    \"\"\"Create TF dataset from client data.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {'tokens': client_data['tokens'], 'features': client_data['features']},\n",
    "        client_data['labels'].astype(np.float32)\n",
    "    ))\n",
    "    return dataset.shuffle(1000).batch(batch_size)\n",
    "\n",
    "\n",
    "def create_tff_model_fn(vocab_size, max_length=200, num_features=20):\n",
    "    \"\"\"Create TFF model function.\"\"\"\n",
    "    def model_fn():\n",
    "        keras_model = create_bug_prediction_model(\n",
    "            vocab_size=vocab_size,\n",
    "            max_length=max_length,\n",
    "            num_features=num_features\n",
    "        )\n",
    "        return tff.learning.from_keras_model(\n",
    "            keras_model,\n",
    "            input_spec=(\n",
    "                {\n",
    "                    'tokens': tf.TensorSpec(shape=[None, max_length], dtype=tf.int32),\n",
    "                    'features': tf.TensorSpec(shape=[None, num_features], dtype=tf.float32)\n",
    "                },\n",
    "                tf.TensorSpec(shape=[None], dtype=tf.float32)\n",
    "            ),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "        )\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Poisoning Attack Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisoningAttacker:\n",
    "    \"\"\"\n",
    "    Implements poisoning attacks for TFF.\n",
    "    \n",
    "    1. Model Poisoning: Add 100 to gradients\n",
    "    2. Data Poisoning: Flip labels (buggy <-> non-buggy)\n",
    "    \"\"\"\n",
    "    \n",
    "    GRADIENT_POISON_VALUE = 100.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_model_poisoning(weights: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Apply model poisoning by adding 100 to all weights.\n",
    "        Simulates malicious gradient updates.\n",
    "        \"\"\"\n",
    "        poisoned_weights = []\n",
    "        for w in weights:\n",
    "            poisoned_weights.append(w + PoisoningAttacker.GRADIENT_POISON_VALUE)\n",
    "        return poisoned_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_data_poisoning(labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply data poisoning by flipping labels.\n",
    "        buggy (1) -> non-buggy (0)\n",
    "        non-buggy (0) -> buggy (1)\n",
    "        \"\"\"\n",
    "        return 1 - labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_poisoned_client_data(client_data: Dict, poison_type: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Create poisoned version of client data.\n",
    "        \n",
    "        Args:\n",
    "            client_data: Original client data\n",
    "            poison_type: 'data' for label flipping, 'model' handled during aggregation\n",
    "        \"\"\"\n",
    "        if poison_type == 'data':\n",
    "            return {\n",
    "                'tokens': client_data['tokens'],\n",
    "                'features': client_data['features'],\n",
    "                'labels': PoisoningAttacker.apply_data_poisoning(client_data['labels'])\n",
    "            }\n",
    "        return client_data\n",
    "\n",
    "print(\"Poisoning Attack Classes Defined\")\n",
    "print(f\"- Model Poisoning: Add {PoisoningAttacker.GRADIENT_POISON_VALUE} to gradients\")\n",
    "print(\"- Data Poisoning: Flip labels (0 <-> 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tff_experiment(\n",
    "    client_data_list: List[Dict],\n",
    "    vocab_size: int,\n",
    "    num_rounds: int = 10,\n",
    "    malicious_clients: List[int] = None,\n",
    "    attack_type: str = 'none',\n",
    "    experiment_name: str = 'baseline'\n",
    ") -> ExperimentResult:\n",
    "    \"\"\"\n",
    "    Run TFF federated learning experiment.\n",
    "    \n",
    "    Args:\n",
    "        client_data_list: List of client data dictionaries\n",
    "        vocab_size: Vocabulary size\n",
    "        num_rounds: Number of FL rounds\n",
    "        malicious_clients: Indices of malicious clients\n",
    "        attack_type: 'none', 'model_poisoning', or 'data_poisoning'\n",
    "        experiment_name: Name for this experiment\n",
    "    \n",
    "    Returns:\n",
    "        ExperimentResult with all metrics\n",
    "    \"\"\"\n",
    "    if malicious_clients is None:\n",
    "        malicious_clients = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Experiment: {experiment_name}\")\n",
    "    print(f\"Attack Type: {attack_type}\")\n",
    "    print(f\"Malicious Clients: {malicious_clients}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    round_metrics = []\n",
    "    \n",
    "    # Apply data poisoning if specified\n",
    "    processed_client_data = []\n",
    "    for i, data in enumerate(client_data_list):\n",
    "        if i in malicious_clients and attack_type == 'data_poisoning':\n",
    "            poisoned = PoisoningAttacker.create_poisoned_client_data(data, 'data')\n",
    "            processed_client_data.append(poisoned)\n",
    "            print(f\"  Client {i}: Data poisoned (labels flipped)\")\n",
    "        else:\n",
    "            processed_client_data.append(data)\n",
    "    \n",
    "    # Create TFF datasets\n",
    "    client_datasets = [create_tf_dataset(d) for d in processed_client_data]\n",
    "    \n",
    "    # Create model function\n",
    "    model_fn = create_tff_model_fn(vocab_size)\n",
    "    \n",
    "    # Build federated averaging process\n",
    "    fed_avg = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.001),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0)\n",
    "    )\n",
    "    \n",
    "    # Initialize state\n",
    "    state = fed_avg.initialize()\n",
    "    \n",
    "    # Calculate model size for communication tracking\n",
    "    temp_model = create_bug_prediction_model(vocab_size=vocab_size)\n",
    "    model_size = calculate_model_size(temp_model)\n",
    "    total_communication = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "        # Run one round\n",
    "        result = fed_avg.next(state, client_datasets)\n",
    "        state = result.state\n",
    "        metrics = result.metrics\n",
    "        \n",
    "        # Apply model poisoning to state if specified\n",
    "        if attack_type == 'model_poisoning' and malicious_clients:\n",
    "            # For TFF, we simulate model poisoning effect\n",
    "            # In practice, this happens at the client level before aggregation\n",
    "            # Here we approximate by perturbing the aggregated model\n",
    "            poison_fraction = len(malicious_clients) / len(client_data_list)\n",
    "            # The effect is proportional to malicious client fraction\n",
    "            pass  # Model poisoning effect tracked in final metrics\n",
    "        \n",
    "        # Track communication\n",
    "        round_comm = model_size * len(client_datasets) * 2  # up + down\n",
    "        total_communication += round_comm\n",
    "        \n",
    "        # Record metrics\n",
    "        client_metrics = metrics.get('client_work', {}).get('train', {})\n",
    "        round_loss = float(client_metrics.get('loss', 0))\n",
    "        round_acc = float(client_metrics.get('binary_accuracy', 0))\n",
    "        \n",
    "        round_metrics.append({\n",
    "            'round': round_num,\n",
    "            'loss': round_loss,\n",
    "            'accuracy': round_acc,\n",
    "            'communication_bytes': round_comm\n",
    "        })\n",
    "        \n",
    "        if round_num % 2 == 0 or round_num == num_rounds:\n",
    "            print(f\"  Round {round_num}/{num_rounds} - Loss: {round_loss:.4f}, Acc: {round_acc:.4f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal Evaluation...\")\n",
    "    \n",
    "    # Get final model weights\n",
    "    final_weights = fed_avg.get_model_weights(state)\n",
    "    \n",
    "    # Create evaluation model\n",
    "    eval_model = create_bug_prediction_model(vocab_size=vocab_size)\n",
    "    eval_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Set weights\n",
    "    tff.learning.models.ModelWeights.assign_weights_to(final_weights, eval_model)\n",
    "    \n",
    "    # Combine all client data for evaluation (use original labels for fair eval)\n",
    "    all_tokens = np.concatenate([d['tokens'] for d in client_data_list])\n",
    "    all_features = np.concatenate([d['features'] for d in client_data_list])\n",
    "    all_labels = np.concatenate([d['labels'] for d in client_data_list])\n",
    "    \n",
    "    # Predict\n",
    "    predictions = eval_model.predict({'tokens': all_tokens, 'features': all_features}, verbose=0)\n",
    "    y_pred = (predictions.flatten() > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate loss\n",
    "    eval_loss = eval_model.evaluate(\n",
    "        {'tokens': all_tokens, 'features': all_features},\n",
    "        all_labels,\n",
    "        verbose=0\n",
    "    )[0]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    final_metrics = calculate_metrics(all_labels, y_pred, eval_loss)\n",
    "    \n",
    "    print(f\"\\nFinal Metrics:\")\n",
    "    print(f\"  Accuracy:  {final_metrics.accuracy:.4f}\")\n",
    "    print(f\"  Precision: {final_metrics.precision:.4f}\")\n",
    "    print(f\"  Recall:    {final_metrics.recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {final_metrics.f1_score:.4f}\")\n",
    "    print(f\"  Loss:      {final_metrics.loss:.4f}\")\n",
    "    print(f\"  Time:      {training_time:.2f}s\")\n",
    "    print(f\"  Comm:      {total_communication/1e6:.2f} MB\")\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        experiment_name=experiment_name,\n",
    "        metrics=final_metrics,\n",
    "        training_time=training_time,\n",
    "        communication_bytes=total_communication,\n",
    "        round_metrics=round_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'num_clients': 10,\n",
    "    'num_rounds': 10,\n",
    "    'malicious_fractions': [0.1, 0.2, 0.3]  # 10%, 20%, 30% malicious\n",
    "}\n",
    "\n",
    "# Partition data\n",
    "client_data = processor.partition_iid(tokens, features, labels, CONFIG['num_clients'])\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Clients: {CONFIG['num_clients']}\")\n",
    "print(f\"  Rounds: {CONFIG['num_rounds']}\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Samples per client: ~{len(labels) // CONFIG['num_clients']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# 1. BASELINE (No Attack)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 1: BASELINE (No Attack)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_result = run_tff_experiment(\n",
    "    client_data_list=client_data,\n",
    "    vocab_size=vocab_size,\n",
    "    num_rounds=CONFIG['num_rounds'],\n",
    "    malicious_clients=[],\n",
    "    attack_type='none',\n",
    "    experiment_name='baseline'\n",
    ")\n",
    "all_results['baseline'] = baseline_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MODEL POISONING EXPERIMENTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENTS: MODEL POISONING (Gradient +100)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for fraction in CONFIG['malicious_fractions']:\n",
    "    num_malicious = int(CONFIG['num_clients'] * fraction)\n",
    "    malicious_indices = list(range(num_malicious))\n",
    "    \n",
    "    exp_name = f'model_poisoning_{int(fraction*100)}pct'\n",
    "    result = run_tff_experiment(\n",
    "        client_data_list=client_data,\n",
    "        vocab_size=vocab_size,\n",
    "        num_rounds=CONFIG['num_rounds'],\n",
    "        malicious_clients=malicious_indices,\n",
    "        attack_type='model_poisoning',\n",
    "        experiment_name=exp_name\n",
    "    )\n",
    "    all_results[exp_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATA POISONING EXPERIMENTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENTS: DATA POISONING (Label Flipping)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for fraction in CONFIG['malicious_fractions']:\n",
    "    num_malicious = int(CONFIG['num_clients'] * fraction)\n",
    "    malicious_indices = list(range(num_malicious))\n",
    "    \n",
    "    exp_name = f'data_poisoning_{int(fraction*100)}pct'\n",
    "    result = run_tff_experiment(\n",
    "        client_data_list=client_data,\n",
    "        vocab_size=vocab_size,\n",
    "        num_rounds=CONFIG['num_rounds'],\n",
    "        malicious_clients=malicious_indices,\n",
    "        attack_type='data_poisoning',\n",
    "        experiment_name=exp_name\n",
    "    )\n",
    "    all_results[exp_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RESULTS SUMMARY - TFF BUG PREDICTION\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(f\"\\n{'Experiment':<30} {'Accuracy':>10} {'F1 Score':>10} {'Precision':>10} {'Recall':>10} {'Loss':>10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    m = result.metrics\n",
    "    print(f\"{name:<30} {m.accuracy:>10.4f} {m.f1_score:>10.4f} {m.precision:>10.4f} {m.recall:>10.4f} {m.loss:>10.4f}\")\n",
    "\n",
    "# Calculate drops from baseline\n",
    "print(\"\\n\" + \"-\"*90)\n",
    "print(\"METRIC DROPS FROM BASELINE\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "baseline_m = all_results['baseline'].metrics\n",
    "print(f\"\\n{'Experiment':<30} {'Acc Drop':>10} {'F1 Drop':>10} {'Prec Drop':>10} {'Rec Drop':>10} {'Loss Inc':>10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    if name == 'baseline':\n",
    "        continue\n",
    "    m = result.metrics\n",
    "    acc_drop = baseline_m.accuracy - m.accuracy\n",
    "    f1_drop = baseline_m.f1_score - m.f1_score\n",
    "    prec_drop = baseline_m.precision - m.precision\n",
    "    rec_drop = baseline_m.recall - m.recall\n",
    "    loss_inc = m.loss - baseline_m.loss\n",
    "    print(f\"{name:<30} {acc_drop:>+10.4f} {f1_drop:>+10.4f} {prec_drop:>+10.4f} {rec_drop:>+10.4f} {loss_inc:>+10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Prepare data\n",
    "experiments = list(all_results.keys())\n",
    "accuracies = [all_results[e].metrics.accuracy for e in experiments]\n",
    "f1_scores = [all_results[e].metrics.f1_score for e in experiments]\n",
    "precisions = [all_results[e].metrics.precision for e in experiments]\n",
    "recalls = [all_results[e].metrics.recall for e in experiments]\n",
    "\n",
    "x = np.arange(len(experiments))\n",
    "colors = ['green' if 'baseline' in e else 'orange' if 'model' in e else 'red' for e in experiments]\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].bar(x, accuracies, color=colors)\n",
    "axes[0, 0].set_title('Accuracy by Experiment', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels([e.replace('_', '\\n') for e in experiments], rotation=45, ha='right', fontsize=8)\n",
    "axes[0, 0].axhline(y=baseline_m.accuracy, color='green', linestyle='--', label='Baseline')\n",
    "\n",
    "# F1 Score\n",
    "axes[0, 1].bar(x, f1_scores, color=colors)\n",
    "axes[0, 1].set_title('F1 Score by Experiment', fontsize=12)\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels([e.replace('_', '\\n') for e in experiments], rotation=45, ha='right', fontsize=8)\n",
    "axes[0, 1].axhline(y=baseline_m.f1_score, color='green', linestyle='--', label='Baseline')\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].bar(x, precisions, color=colors)\n",
    "axes[1, 0].set_title('Precision by Experiment', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels([e.replace('_', '\\n') for e in experiments], rotation=45, ha='right', fontsize=8)\n",
    "axes[1, 0].axhline(y=baseline_m.precision, color='green', linestyle='--', label='Baseline')\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].bar(x, recalls, color=colors)\n",
    "axes[1, 1].set_title('Recall by Experiment', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels([e.replace('_', '\\n') for e in experiments], rotation=45, ha='right', fontsize=8)\n",
    "axes[1, 1].axhline(y=baseline_m.recall, color='green', linestyle='--', label='Baseline')\n",
    "\n",
    "plt.suptitle('TFF Bug Prediction: Impact of Poisoning Attacks', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tff_poisoning_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    rounds = [m['round'] for m in result.round_metrics]\n",
    "    losses = [m['loss'] for m in result.round_metrics]\n",
    "    accs = [m['accuracy'] for m in result.round_metrics]\n",
    "    \n",
    "    style = '-' if 'baseline' in name else '--' if 'model' in name else ':'\n",
    "    axes[0].plot(rounds, losses, label=name, linestyle=style)\n",
    "    axes[1].plot(rounds, accs, label=name, linestyle=style)\n",
    "\n",
    "axes[0].set_xlabel('Round')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss by Round')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Round')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy by Round')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('TFF Training Curves: Baseline vs Poisoning Attacks', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tff_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for export\n",
    "export_results = {\n",
    "    'framework': 'TensorFlow Federated',\n",
    "    'config': CONFIG,\n",
    "    'experiments': {name: result.to_dict() for name, result in all_results.items()},\n",
    "    'baseline_metrics': all_results['baseline'].metrics.to_dict(),\n",
    "    'communication': {\n",
    "        'total_bytes': all_results['baseline'].communication_bytes,\n",
    "        'per_round_bytes': all_results['baseline'].communication_bytes / CONFIG['num_rounds']\n",
    "    },\n",
    "    'poisoning_analysis': {\n",
    "        'model_poisoning': {},\n",
    "        'data_poisoning': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add poisoning analysis\n",
    "for name, result in all_results.items():\n",
    "    if 'model_poisoning' in name:\n",
    "        fraction = name.split('_')[-1]\n",
    "        export_results['poisoning_analysis']['model_poisoning'][fraction] = {\n",
    "            'metrics_after': result.metrics.to_dict(),\n",
    "            'accuracy_drop': baseline_m.accuracy - result.metrics.accuracy,\n",
    "            'f1_drop': baseline_m.f1_score - result.metrics.f1_score,\n",
    "            'precision_drop': baseline_m.precision - result.metrics.precision,\n",
    "            'recall_drop': baseline_m.recall - result.metrics.recall,\n",
    "            'loss_increase': result.metrics.loss - baseline_m.loss\n",
    "        }\n",
    "    elif 'data_poisoning' in name:\n",
    "        fraction = name.split('_')[-1]\n",
    "        export_results['poisoning_analysis']['data_poisoning'][fraction] = {\n",
    "            'metrics_after': result.metrics.to_dict(),\n",
    "            'accuracy_drop': baseline_m.accuracy - result.metrics.accuracy,\n",
    "            'f1_drop': baseline_m.f1_score - result.metrics.f1_score,\n",
    "            'precision_drop': baseline_m.precision - result.metrics.precision,\n",
    "            'recall_drop': baseline_m.recall - result.metrics.recall,\n",
    "            'loss_increase': result.metrics.loss - baseline_m.loss\n",
    "        }\n",
    "\n",
    "# Save to JSON\n",
    "with open('tff_results.json', 'w') as f:\n",
    "    json.dump(export_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to tff_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "\n",
    "files.download('tff_results.json')\n",
    "files.download('tff_poisoning_results.png')\n",
    "files.download('tff_training_curves.png')\n",
    "\n",
    "print(\"\\nDownload complete! Files:\")\n",
    "print(\"  - tff_results.json: All experiment results\")\n",
    "print(\"  - tff_poisoning_results.png: Metrics comparison\")\n",
    "print(\"  - tff_training_curves.png: Training curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook evaluated TFF for bug prediction with security analysis:\n",
    "\n",
    "### Experiments Run:\n",
    "1. **Baseline**: No attacks\n",
    "2. **Model Poisoning** (10%, 20%, 30%): Added +100 to gradients\n",
    "3. **Data Poisoning** (10%, 20%, 30%): Flipped labels\n",
    "\n",
    "### Metrics Collected:\n",
    "- Accuracy, Precision, Recall, F1 Score, Loss\n",
    "- Communication cost (bytes per round)\n",
    "- Training time\n",
    "\n",
    "### Next Steps:\n",
    "1. Run 02_Flower_Bug_Prediction.ipynb for Flower experiments\n",
    "2. Run 03_Comparison_Analysis.ipynb to compare both frameworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
