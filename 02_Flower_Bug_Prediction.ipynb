{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flower Framework Bug Prediction Experiment\n",
    "\n",
    "## Study: FL Framework Comparison - TFF vs Flower\n",
    "\n",
    "This notebook evaluates Flower for bug prediction with:\n",
    "- **Communication Cost**: Tracking bytes sent/received per round\n",
    "- **Security Analysis**: Model poisoning (gradient +100) and Data poisoning (label flipping)\n",
    "- **Metrics**: F1 Score, Accuracy, Precision, Recall, Loss (before/after attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install flwr>=1.5.0 torch>=2.0.0 scikit-learn -q\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import flwr as fl\n",
    "from flwr.common import NDArrays, Scalar\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Flower version: {fl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload dataset_pairs_1_.json\")\n",
    "uploaded = files.upload()\n",
    "dataset_path = list(uploaded.keys())[0]\n",
    "print(f\"\\nUploaded: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTokenizer:\n",
    "    \"\"\"Simple tokenizer for Python code.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=5000, max_length=200):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}\n",
    "        self.word_counts = {}\n",
    "    \n",
    "    def fit(self, codes: List[str]):\n",
    "        for code in codes:\n",
    "            tokens = self._tokenize(code)\n",
    "            for token in tokens:\n",
    "                self.word_counts[token] = self.word_counts.get(token, 0) + 1\n",
    "        \n",
    "        sorted_words = sorted(self.word_counts.items(), key=lambda x: -x[1])\n",
    "        for word, _ in sorted_words[:self.max_vocab_size - len(self.vocab)]:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def _tokenize(self, code: str) -> List[str]:\n",
    "        import re\n",
    "        tokens = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*|[0-9]+|[^\\s\\w]', code)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, code: str) -> np.ndarray:\n",
    "        tokens = self._tokenize(code)\n",
    "        ids = [self.vocab.get(t, 1) for t in tokens[:self.max_length]]\n",
    "        padded = ids + [0] * (self.max_length - len(ids))\n",
    "        return np.array(padded[:self.max_length], dtype=np.int64)\n",
    "\n",
    "\n",
    "class CodeFeatureExtractor:\n",
    "    \"\"\"Extract handcrafted features from code.\"\"\"\n",
    "    \n",
    "    def extract(self, code: str) -> np.ndarray:\n",
    "        features = [\n",
    "            len(code), code.count('\\n'), code.count('def '),\n",
    "            code.count('class '), code.count('if '), code.count('for '),\n",
    "            code.count('while '), code.count('try:'), code.count('except'),\n",
    "            code.count('return'), code.count('import'), code.count('='),\n",
    "            code.count('+'), code.count('-'), code.count('*'),\n",
    "            code.count('/'), code.count('['), code.count('('),\n",
    "            len(code.split()), code.count('#')\n",
    "        ]\n",
    "        return np.array(features, dtype=np.float32)\n",
    "\n",
    "\n",
    "class BugPredictionDataProcessor:\n",
    "    \"\"\"Process bug prediction dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: CodeTokenizer, feature_extractor: CodeFeatureExtractor):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "    \n",
    "    def load_data(self, json_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        all_codes = []\n",
    "        for pair in data:\n",
    "            all_codes.append(pair['buggy_code'])\n",
    "            all_codes.append(pair['fixed_code'])\n",
    "        \n",
    "        self.tokenizer.fit(all_codes)\n",
    "        \n",
    "        tokens_list, features_list, labels_list = [], [], []\n",
    "        \n",
    "        for pair in data:\n",
    "            # Buggy code (label=1)\n",
    "            tokens_list.append(self.tokenizer.encode(pair['buggy_code']))\n",
    "            features_list.append(self.feature_extractor.extract(pair['buggy_code']))\n",
    "            labels_list.append(1)\n",
    "            \n",
    "            # Fixed code (label=0)\n",
    "            tokens_list.append(self.tokenizer.encode(pair['fixed_code']))\n",
    "            features_list.append(self.feature_extractor.extract(pair['fixed_code']))\n",
    "            labels_list.append(0)\n",
    "        \n",
    "        return np.array(tokens_list), np.array(features_list), np.array(labels_list)\n",
    "    \n",
    "    def partition_iid(self, tokens, features, labels, num_clients):\n",
    "        indices = np.random.permutation(len(labels))\n",
    "        splits = np.array_split(indices, num_clients)\n",
    "        \n",
    "        client_data = []\n",
    "        for split in splits:\n",
    "            client_data.append({\n",
    "                'tokens': tokens[split],\n",
    "                'features': features[split],\n",
    "                'labels': labels[split]\n",
    "            })\n",
    "        return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "tokenizer = CodeTokenizer(max_vocab_size=5000, max_length=200)\n",
    "feature_extractor = CodeFeatureExtractor()\n",
    "processor = BugPredictionDataProcessor(tokenizer, feature_extractor)\n",
    "\n",
    "tokens, features, labels = processor.load_data(dataset_path)\n",
    "print(f\"Total samples: {len(labels)}\")\n",
    "print(f\"Buggy (1): {sum(labels)}, Non-buggy (0): {len(labels) - sum(labels)}\")\n",
    "print(f\"Token shape: {tokens.shape}, Features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugPredictionModel(nn.Module):\n",
    "    \"\"\"PyTorch model for bug prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=5000, embedding_dim=64, hidden_dim=128, num_features=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token processing\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Feature processing\n",
    "        self.feature_fc = nn.Linear(num_features, 32)\n",
    "        \n",
    "        # Combined layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 + 32, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, tokens, features):\n",
    "        # Token branch\n",
    "        embedded = self.embedding(tokens)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take last output\n",
    "        \n",
    "        # Feature branch\n",
    "        feat_out = F.relu(self.feature_fc(features))\n",
    "        \n",
    "        # Combine\n",
    "        combined = torch.cat([lstm_out, feat_out], dim=1)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "def get_model_parameters(model: nn.Module) -> List[np.ndarray]:\n",
    "    \"\"\"Extract model parameters as numpy arrays.\"\"\"\n",
    "    return [param.cpu().detach().numpy() for param in model.parameters()]\n",
    "\n",
    "\n",
    "def set_model_parameters(model: nn.Module, parameters: List[np.ndarray]):\n",
    "    \"\"\"Set model parameters from numpy arrays.\"\"\"\n",
    "    for param, new_param in zip(model.parameters(), parameters):\n",
    "        param.data = torch.tensor(new_param, dtype=param.dtype, device=param.device)\n",
    "\n",
    "\n",
    "def calculate_model_size(model: nn.Module) -> int:\n",
    "    \"\"\"Calculate model size in bytes.\"\"\"\n",
    "    total_bytes = 0\n",
    "    for param in model.parameters():\n",
    "        total_bytes += param.numel() * param.element_size()\n",
    "    return total_bytes\n",
    "\n",
    "\n",
    "# Test model\n",
    "test_model = BugPredictionModel(vocab_size=len(tokenizer.vocab))\n",
    "print(f\"Model parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "print(f\"Model size: {calculate_model_size(test_model) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClassificationMetrics:\n",
    "    \"\"\"Store classification metrics.\"\"\"\n",
    "    accuracy: float = 0.0\n",
    "    precision: float = 0.0\n",
    "    recall: float = 0.0\n",
    "    f1_score: float = 0.0\n",
    "    loss: float = 0.0\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Store experiment results.\"\"\"\n",
    "    experiment_name: str\n",
    "    metrics: ClassificationMetrics\n",
    "    training_time: float = 0.0\n",
    "    communication_bytes: float = 0.0\n",
    "    round_metrics: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'experiment_name': self.experiment_name,\n",
    "            'metrics': self.metrics.to_dict(),\n",
    "            'training_time': self.training_time,\n",
    "            'communication_bytes': self.communication_bytes,\n",
    "            'round_metrics': self.round_metrics\n",
    "        }\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, loss=0.0) -> ClassificationMetrics:\n",
    "    \"\"\"Calculate all classification metrics.\"\"\"\n",
    "    return ClassificationMetrics(\n",
    "        accuracy=accuracy_score(y_true, y_pred),\n",
    "        precision=precision_score(y_true, y_pred, zero_division=0),\n",
    "        recall=recall_score(y_true, y_pred, zero_division=0),\n",
    "        f1_score=f1_score(y_true, y_pred, zero_division=0),\n",
    "        loss=loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Poisoning Attack Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisoningAttacker:\n",
    "    \"\"\"\n",
    "    Implements poisoning attacks for Flower.\n",
    "    \n",
    "    1. Model Poisoning: Add 100 to gradients/weights\n",
    "    2. Data Poisoning: Flip labels (buggy <-> non-buggy)\n",
    "    \"\"\"\n",
    "    \n",
    "    GRADIENT_POISON_VALUE = 100.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_model_poisoning(parameters: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Apply model poisoning by adding 100 to all parameters.\n",
    "        Simulates malicious gradient/weight updates.\n",
    "        \"\"\"\n",
    "        poisoned = []\n",
    "        for param in parameters:\n",
    "            poisoned.append(param + PoisoningAttacker.GRADIENT_POISON_VALUE)\n",
    "        return poisoned\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_data_poisoning(labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply data poisoning by flipping labels.\n",
    "        buggy (1) -> non-buggy (0)\n",
    "        non-buggy (0) -> buggy (1)\n",
    "        \"\"\"\n",
    "        return 1 - labels\n",
    "\n",
    "\n",
    "print(\"Poisoning Attack Classes Defined\")\n",
    "print(f\"- Model Poisoning: Add {PoisoningAttacker.GRADIENT_POISON_VALUE} to parameters\")\n",
    "print(\"- Data Poisoning: Flip labels (0 <-> 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Flower Client Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerBugClient:\n",
    "    \"\"\"Flower client for bug prediction with optional poisoning.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        model: nn.Module,\n",
    "        train_data: Dict,\n",
    "        is_malicious: bool = False,\n",
    "        attack_type: str = 'none'\n",
    "    ):\n",
    "        self.client_id = client_id\n",
    "        self.model = model\n",
    "        self.is_malicious = is_malicious\n",
    "        self.attack_type = attack_type\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        # Prepare data\n",
    "        labels = train_data['labels'].copy()\n",
    "        if is_malicious and attack_type == 'data_poisoning':\n",
    "            labels = PoisoningAttacker.apply_data_poisoning(labels)\n",
    "            print(f\"  Client {client_id}: Data poisoned (labels flipped)\")\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            TensorDataset(\n",
    "                torch.tensor(train_data['tokens'], dtype=torch.long),\n",
    "                torch.tensor(train_data['features'], dtype=torch.float32),\n",
    "                torch.tensor(labels, dtype=torch.float32)\n",
    "            ),\n",
    "            batch_size=32,\n",
    "            shuffle=True\n",
    "        )\n",
    "    \n",
    "    def get_parameters(self) -> List[np.ndarray]:\n",
    "        return get_model_parameters(self.model)\n",
    "    \n",
    "    def set_parameters(self, parameters: List[np.ndarray]):\n",
    "        set_model_parameters(self.model, parameters)\n",
    "    \n",
    "    def train(self, epochs: int = 1) -> Tuple[List[np.ndarray], int, Dict]:\n",
    "        \"\"\"Train model and return updated parameters.\"\"\"\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for tokens, features, labels in self.train_loader:\n",
    "                tokens = tokens.to(self.device)\n",
    "                features = features.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(tokens, features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        # Get updated parameters\n",
    "        parameters = self.get_parameters()\n",
    "        \n",
    "        # Apply model poisoning if malicious\n",
    "        if self.is_malicious and self.attack_type == 'model_poisoning':\n",
    "            parameters = PoisoningAttacker.apply_model_poisoning(parameters)\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        \n",
    "        return parameters, len(self.train_loader.dataset), {'loss': avg_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Federated Learning Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_averaging(client_updates: List[Tuple[List[np.ndarray], int]]) -> List[np.ndarray]:\n",
    "    \"\"\"Perform FedAvg aggregation.\"\"\"\n",
    "    total_samples = sum(num_samples for _, num_samples, _ in client_updates)\n",
    "    \n",
    "    # Initialize with zeros\n",
    "    aggregated = [np.zeros_like(client_updates[0][0][i]) for i in range(len(client_updates[0][0]))]\n",
    "    \n",
    "    # Weighted average\n",
    "    for params, num_samples, _ in client_updates:\n",
    "        weight = num_samples / total_samples\n",
    "        for i, param in enumerate(params):\n",
    "            aggregated[i] += param * weight\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def run_flower_experiment(\n",
    "    client_data_list: List[Dict],\n",
    "    vocab_size: int,\n",
    "    num_rounds: int = 10,\n",
    "    malicious_clients: List[int] = None,\n",
    "    attack_type: str = 'none',\n",
    "    experiment_name: str = 'baseline'\n",
    ") -> ExperimentResult:\n",
    "    \"\"\"\n",
    "    Run Flower federated learning experiment.\n",
    "    \n",
    "    Args:\n",
    "        client_data_list: List of client data dictionaries\n",
    "        vocab_size: Vocabulary size\n",
    "        num_rounds: Number of FL rounds\n",
    "        malicious_clients: Indices of malicious clients\n",
    "        attack_type: 'none', 'model_poisoning', or 'data_poisoning'\n",
    "        experiment_name: Name for this experiment\n",
    "    \n",
    "    Returns:\n",
    "        ExperimentResult with all metrics\n",
    "    \"\"\"\n",
    "    if malicious_clients is None:\n",
    "        malicious_clients = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Experiment: {experiment_name}\")\n",
    "    print(f\"Attack Type: {attack_type}\")\n",
    "    print(f\"Malicious Clients: {malicious_clients}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    round_metrics = []\n",
    "    \n",
    "    # Create global model\n",
    "    global_model = BugPredictionModel(vocab_size=vocab_size).to(device)\n",
    "    model_size = calculate_model_size(global_model)\n",
    "    total_communication = 0\n",
    "    \n",
    "    # Create clients\n",
    "    clients = []\n",
    "    for i, data in enumerate(client_data_list):\n",
    "        is_malicious = i in malicious_clients\n",
    "        client_model = BugPredictionModel(vocab_size=vocab_size).to(device)\n",
    "        client = FlowerBugClient(\n",
    "            client_id=i,\n",
    "            model=client_model,\n",
    "            train_data=data,\n",
    "            is_malicious=is_malicious,\n",
    "            attack_type=attack_type\n",
    "        )\n",
    "        clients.append(client)\n",
    "    \n",
    "    # Get initial global parameters\n",
    "    global_params = get_model_parameters(global_model)\n",
    "    \n",
    "    # Training loop\n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "        # Distribute global model to clients\n",
    "        for client in clients:\n",
    "            client.set_parameters(global_params)\n",
    "        \n",
    "        # Client training\n",
    "        client_updates = []\n",
    "        round_losses = []\n",
    "        \n",
    "        for client in clients:\n",
    "            params, num_samples, metrics = client.train(epochs=1)\n",
    "            client_updates.append((params, num_samples, metrics))\n",
    "            round_losses.append(metrics['loss'])\n",
    "        \n",
    "        # Aggregate\n",
    "        global_params = federated_averaging(client_updates)\n",
    "        set_model_parameters(global_model, global_params)\n",
    "        \n",
    "        # Track communication (upload + download for all clients)\n",
    "        round_comm = model_size * len(clients) * 2\n",
    "        total_communication += round_comm\n",
    "        \n",
    "        # Quick evaluation\n",
    "        global_model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in client_data_list:\n",
    "                tokens_t = torch.tensor(data['tokens'], dtype=torch.long).to(device)\n",
    "                features_t = torch.tensor(data['features'], dtype=torch.float32).to(device)\n",
    "                outputs = global_model(tokens_t, features_t)\n",
    "                preds = (outputs > 0.5).cpu().numpy().astype(int)\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(data['labels'])\n",
    "        \n",
    "        round_acc = accuracy_score(all_labels, all_preds)\n",
    "        round_loss = np.mean(round_losses)\n",
    "        \n",
    "        round_metrics.append({\n",
    "            'round': round_num,\n",
    "            'loss': round_loss,\n",
    "            'accuracy': round_acc,\n",
    "            'communication_bytes': round_comm\n",
    "        })\n",
    "        \n",
    "        if round_num % 2 == 0 or round_num == num_rounds:\n",
    "            print(f\"  Round {round_num}/{num_rounds} - Loss: {round_loss:.4f}, Acc: {round_acc:.4f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal Evaluation...\")\n",
    "    global_model.eval()\n",
    "    \n",
    "    all_tokens = np.concatenate([d['tokens'] for d in client_data_list])\n",
    "    all_features = np.concatenate([d['features'] for d in client_data_list])\n",
    "    all_labels = np.concatenate([d['labels'] for d in client_data_list])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tokens_t = torch.tensor(all_tokens, dtype=torch.long).to(device)\n",
    "        features_t = torch.tensor(all_features, dtype=torch.float32).to(device)\n",
    "        labels_t = torch.tensor(all_labels, dtype=torch.float32).to(device)\n",
    "        \n",
    "        outputs = global_model(tokens_t, features_t)\n",
    "        y_pred = (outputs > 0.5).cpu().numpy().astype(int)\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        eval_loss = criterion(outputs, labels_t).item()\n",
    "    \n",
    "    final_metrics = calculate_metrics(all_labels, y_pred, eval_loss)\n",
    "    \n",
    "    print(f\"\\nFinal Metrics:\")\n",
    "    print(f\"  Accuracy:  {final_metrics.accuracy:.4f}\")\n",
    "    print(f\"  Precision: {final_metrics.precision:.4f}\")\n",
    "    print(f\"  Recall:    {final_metrics.recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {final_metrics.f1_score:.4f}\")\n",
    "    print(f\"  Loss:      {final_metrics.loss:.4f}\")\n",
    "    print(f\"  Time:      {training_time:.2f}s\")\n",
    "    print(f\"  Comm:      {total_communication/1e6:.2f} MB\")\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        experiment_name=experiment_name,\n",
    "        metrics=final_metrics,\n",
    "        training_time=training_time,\n",
    "        communication_bytes=total_communication,\n",
    "        round_metrics=round_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'num_clients': 10,\n",
    "    'num_rounds': 10,\n",
    "    'malicious_fractions': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Partition data\n",
    "client_data = processor.partition_iid(tokens, features, labels, CONFIG['num_clients'])\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Clients: {CONFIG['num_clients']}\")\n",
    "print(f\"  Rounds: {CONFIG['num_rounds']}\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Samples per client: ~{len(labels) // CONFIG['num_clients']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# 1. BASELINE\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 1: BASELINE (No Attack)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_result = run_flower_experiment(\n",
    "    client_data_list=client_data,\n",
    "    vocab_size=vocab_size,\n",
    "    num_rounds=CONFIG['num_rounds'],\n",
    "    malicious_clients=[],\n",
    "    attack_type='none',\n",
    "    experiment_name='baseline'\n",
    ")\n",
    "all_results['baseline'] = baseline_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MODEL POISONING\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENTS: MODEL POISONING (Parameter +100)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for fraction in CONFIG['malicious_fractions']:\n",
    "    num_malicious = int(CONFIG['num_clients'] * fraction)\n",
    "    malicious_indices = list(range(num_malicious))\n",
    "    \n",
    "    exp_name = f'model_poisoning_{int(fraction*100)}pct'\n",
    "    result = run_flower_experiment(\n",
    "        client_data_list=client_data,\n",
    "        vocab_size=vocab_size,\n",
    "        num_rounds=CONFIG['num_rounds'],\n",
    "        malicious_clients=malicious_indices,\n",
    "        attack_type='model_poisoning',\n",
    "        experiment_name=exp_name\n",
    "    )\n",
    "    all_results[exp_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATA POISONING\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENTS: DATA POISONING (Label Flipping)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for fraction in CONFIG['malicious_fractions']:\n",
    "    num_malicious = int(CONFIG['num_clients'] * fraction)\n",
    "    malicious_indices = list(range(num_malicious))\n",
    "    \n",
    "    exp_name = f'data_poisoning_{int(fraction*100)}pct'\n",
    "    result = run_flower_experiment(\n",
    "        client_data_list=client_data,\n",
    "        vocab_size=vocab_size,\n",
    "        num_rounds=CONFIG['num_rounds'],\n",
    "        malicious_clients=malicious_indices,\n",
    "        attack_type='data_poisoning',\n",
    "        experiment_name=exp_name\n",
    "    )\n",
    "    all_results[exp_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RESULTS SUMMARY - FLOWER BUG PREDICTION\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(f\"\\n{'Experiment':<30} {'Accuracy':>10} {'F1 Score':>10} {'Precision':>10} {'Recall':>10} {'Loss':>10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    m = result.metrics\n",
    "    print(f\"{name:<30} {m.accuracy:>10.4f} {m.f1_score:>10.4f} {m.precision:>10.4f} {m.recall:>10.4f} {m.loss:>10.4f}\")\n",
    "\n",
    "# Calculate drops\n",
    "print(\"\\n\" + \"-\"*90)\n",
    "print(\"METRIC DROPS FROM BASELINE\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "baseline_m = all_results['baseline'].metrics\n",
    "print(f\"\\n{'Experiment':<30} {'Acc Drop':>10} {'F1 Drop':>10} {'Prec Drop':>10} {'Rec Drop':>10} {'Loss Inc':>10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    if name == 'baseline':\n",
    "        continue\n",
    "    m = result.metrics\n",
    "    print(f\"{name:<30} {baseline_m.accuracy - m.accuracy:>+10.4f} {baseline_m.f1_score - m.f1_score:>+10.4f} {baseline_m.precision - m.precision:>+10.4f} {baseline_m.recall - m.recall:>+10.4f} {m.loss - baseline_m.loss:>+10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "experiments = list(all_results.keys())\n",
    "accuracies = [all_results[e].metrics.accuracy for e in experiments]\n",
    "f1_scores = [all_results[e].metrics.f1_score for e in experiments]\n",
    "precisions = [all_results[e].metrics.precision for e in experiments]\n",
    "recalls = [all_results[e].metrics.recall for e in experiments]\n",
    "\n",
    "x = np.arange(len(experiments))\n",
    "colors = ['green' if 'baseline' in e else 'orange' if 'model' in e else 'red' for e in experiments]\n",
    "\n",
    "for ax, data, title in zip(axes.flat, \n",
    "                           [accuracies, f1_scores, precisions, recalls],\n",
    "                           ['Accuracy', 'F1 Score', 'Precision', 'Recall']):\n",
    "    ax.bar(x, data, color=colors)\n",
    "    ax.set_title(f'{title} by Experiment', fontsize=12)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([e.replace('_', '\\n') for e in experiments], rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "plt.suptitle('Flower Bug Prediction: Impact of Poisoning Attacks', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('flower_poisoning_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    rounds = [m['round'] for m in result.round_metrics]\n",
    "    losses = [m['loss'] for m in result.round_metrics]\n",
    "    accs = [m['accuracy'] for m in result.round_metrics]\n",
    "    \n",
    "    style = '-' if 'baseline' in name else '--' if 'model' in name else ':'\n",
    "    axes[0].plot(rounds, losses, label=name, linestyle=style)\n",
    "    axes[1].plot(rounds, accs, label=name, linestyle=style)\n",
    "\n",
    "axes[0].set_xlabel('Round')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss by Round')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Round')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy by Round')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Flower Training Curves: Baseline vs Poisoning Attacks', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('flower_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare export\n",
    "export_results = {\n",
    "    'framework': 'Flower',\n",
    "    'config': CONFIG,\n",
    "    'experiments': {name: result.to_dict() for name, result in all_results.items()},\n",
    "    'baseline_metrics': all_results['baseline'].metrics.to_dict(),\n",
    "    'communication': {\n",
    "        'total_bytes': all_results['baseline'].communication_bytes,\n",
    "        'per_round_bytes': all_results['baseline'].communication_bytes / CONFIG['num_rounds']\n",
    "    },\n",
    "    'poisoning_analysis': {\n",
    "        'model_poisoning': {},\n",
    "        'data_poisoning': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    if 'model_poisoning' in name:\n",
    "        fraction = name.split('_')[-1]\n",
    "        export_results['poisoning_analysis']['model_poisoning'][fraction] = {\n",
    "            'metrics_after': result.metrics.to_dict(),\n",
    "            'accuracy_drop': baseline_m.accuracy - result.metrics.accuracy,\n",
    "            'f1_drop': baseline_m.f1_score - result.metrics.f1_score,\n",
    "            'precision_drop': baseline_m.precision - result.metrics.precision,\n",
    "            'recall_drop': baseline_m.recall - result.metrics.recall,\n",
    "            'loss_increase': result.metrics.loss - baseline_m.loss\n",
    "        }\n",
    "    elif 'data_poisoning' in name:\n",
    "        fraction = name.split('_')[-1]\n",
    "        export_results['poisoning_analysis']['data_poisoning'][fraction] = {\n",
    "            'metrics_after': result.metrics.to_dict(),\n",
    "            'accuracy_drop': baseline_m.accuracy - result.metrics.accuracy,\n",
    "            'f1_drop': baseline_m.f1_score - result.metrics.f1_score,\n",
    "            'precision_drop': baseline_m.precision - result.metrics.precision,\n",
    "            'recall_drop': baseline_m.recall - result.metrics.recall,\n",
    "            'loss_increase': result.metrics.loss - baseline_m.loss\n",
    "        }\n",
    "\n",
    "with open('flower_results.json', 'w') as f:\n",
    "    json.dump(export_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to flower_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "\n",
    "files.download('flower_results.json')\n",
    "files.download('flower_poisoning_results.png')\n",
    "files.download('flower_training_curves.png')\n",
    "\n",
    "print(\"\\nDownload complete! Files:\")\n",
    "print(\"  - flower_results.json\")\n",
    "print(\"  - flower_poisoning_results.png\")\n",
    "print(\"  - flower_training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook evaluated Flower for bug prediction with security analysis:\n",
    "\n",
    "### Experiments Run:\n",
    "1. **Baseline**: No attacks\n",
    "2. **Model Poisoning** (10%, 20%, 30%): Added +100 to parameters\n",
    "3. **Data Poisoning** (10%, 20%, 30%): Flipped labels\n",
    "\n",
    "### Metrics Collected:\n",
    "- Accuracy, Precision, Recall, F1 Score, Loss\n",
    "- Communication cost (bytes per round)\n",
    "- Training time\n",
    "\n",
    "### Next Steps:\n",
    "Run 03_Comparison_Analysis.ipynb to compare TFF and Flower results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
