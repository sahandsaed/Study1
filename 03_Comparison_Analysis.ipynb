{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework Comparison Analysis: TFF vs Flower\n",
    "\n",
    "## Technical Factors Evaluation\n",
    "\n",
    "This notebook compares TensorFlow Federated and Flower based on:\n",
    "\n",
    "### 1. Communication Cost\n",
    "- Total bytes sent/received\n",
    "- Per-round communication overhead\n",
    "\n",
    "### 2. Security (Poisoning Attack Robustness)\n",
    "- **Model Poisoning**: Adding +100 to gradients\n",
    "- **Data Poisoning**: Flipping labels (buggy ↔ non-buggy)\n",
    "- **Metrics**: F1 Score, Accuracy, Precision, Recall, Loss (before/after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install matplotlib numpy pandas scipy -q\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Results Files\n",
    "\n",
    "Upload the results from TFF and Flower experiments:\n",
    "- `tff_results.json` (from notebook 01)\n",
    "- `flower_results.json` (from notebook 02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload tff_results.json and flower_results.json\")\n",
    "print(\"(You can select both files at once)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(f\"\\nUploaded {len(uploaded)} file(s): {list(uploaded.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open('tff_results.json', 'r') as f:\n",
    "    tff_results = json.load(f)\n",
    "\n",
    "with open('flower_results.json', 'r') as f:\n",
    "    flower_results = json.load(f)\n",
    "\n",
    "print(\"TFF Results loaded:\")\n",
    "print(f\"  - Framework: {tff_results['framework']}\")\n",
    "print(f\"  - Experiments: {list(tff_results['experiments'].keys())}\")\n",
    "\n",
    "print(\"\\nFlower Results loaded:\")\n",
    "print(f\"  - Framework: {flower_results['framework']}\")\n",
    "print(f\"  - Experiments: {list(flower_results['experiments'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract baseline metrics\n",
    "tff_baseline = tff_results['baseline_metrics']\n",
    "flower_baseline = flower_results['baseline_metrics']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BASELINE METRICS COMPARISON (Before Any Attacks)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<20} {'TFF':>15} {'Flower':>15} {'Difference':>15}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "metrics = ['accuracy', 'f1_score', 'precision', 'recall', 'loss']\n",
    "for metric in metrics:\n",
    "    tff_val = tff_baseline[metric]\n",
    "    flower_val = flower_baseline[metric]\n",
    "    diff = flower_val - tff_val\n",
    "    print(f\"{metric:<20} {tff_val:>15.4f} {flower_val:>15.4f} {diff:>+15.4f}\")\n",
    "\n",
    "# Determine which is better for each metric\n",
    "print(\"\\n\" + \"-\"*65)\n",
    "print(\"Summary:\")\n",
    "better_acc = \"Flower\" if flower_baseline['accuracy'] > tff_baseline['accuracy'] else \"TFF\"\n",
    "better_f1 = \"Flower\" if flower_baseline['f1_score'] > tff_baseline['f1_score'] else \"TFF\"\n",
    "print(f\"  Better Accuracy: {better_acc}\")\n",
    "print(f\"  Better F1 Score: {better_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Communication Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract communication data\n",
    "tff_comm = tff_results['communication']\n",
    "flower_comm = flower_results['communication']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMMUNICATION COST COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<30} {'TFF':>18} {'Flower':>18}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Total Communication (MB)':<30} {tff_comm['total_bytes']/1e6:>18.2f} {flower_comm['total_bytes']/1e6:>18.2f}\")\n",
    "print(f\"{'Per Round (KB)':<30} {tff_comm['per_round_bytes']/1e3:>18.2f} {flower_comm['per_round_bytes']/1e3:>18.2f}\")\n",
    "\n",
    "# Calculate difference\n",
    "comm_diff_pct = ((tff_comm['total_bytes'] - flower_comm['total_bytes']) / flower_comm['total_bytes']) * 100\n",
    "print(f\"\\n→ Flower uses {abs(comm_diff_pct):.1f}% {'less' if comm_diff_pct > 0 else 'more'} bandwidth than TFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize communication cost\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "frameworks = ['TFF', 'Flower']\n",
    "total_comm = [tff_comm['total_bytes']/1e6, flower_comm['total_bytes']/1e6]\n",
    "\n",
    "bars = ax.bar(frameworks, total_comm, color=['#2196F3', '#4CAF50'], width=0.5)\n",
    "ax.set_ylabel('Total Communication (MB)')\n",
    "ax.set_title('Communication Cost Comparison: TFF vs Flower')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, total_comm):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            f'{val:.2f} MB', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('communication_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Security Analysis: Model Poisoning (Gradient +100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model poisoning results\n",
    "tff_model_poison = tff_results['poisoning_analysis']['model_poisoning']\n",
    "flower_model_poison = flower_results['poisoning_analysis']['model_poisoning']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECURITY ANALYSIS: MODEL POISONING (Gradient +100)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TFF - Model Poisoning Results\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Malicious %':<15} {'Accuracy':>12} {'F1 Score':>12} {'Precision':>12} {'Recall':>12} {'Loss':>12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Baseline':<15} {tff_baseline['accuracy']:>12.4f} {tff_baseline['f1_score']:>12.4f} {tff_baseline['precision']:>12.4f} {tff_baseline['recall']:>12.4f} {tff_baseline['loss']:>12.4f}\")\n",
    "\n",
    "for pct, data in sorted(tff_model_poison.items()):\n",
    "    m = data['metrics_after']\n",
    "    print(f\"{pct:<15} {m['accuracy']:>12.4f} {m['f1_score']:>12.4f} {m['precision']:>12.4f} {m['recall']:>12.4f} {m['loss']:>12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Flower - Model Poisoning Results\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Malicious %':<15} {'Accuracy':>12} {'F1 Score':>12} {'Precision':>12} {'Recall':>12} {'Loss':>12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Baseline':<15} {flower_baseline['accuracy']:>12.4f} {flower_baseline['f1_score']:>12.4f} {flower_baseline['precision']:>12.4f} {flower_baseline['recall']:>12.4f} {flower_baseline['loss']:>12.4f}\")\n",
    "\n",
    "for pct, data in sorted(flower_model_poison.items()):\n",
    "    m = data['metrics_after']\n",
    "    print(f\"{pct:<15} {m['accuracy']:>12.4f} {m['f1_score']:>12.4f} {m['precision']:>12.4f} {m['recall']:>12.4f} {m['loss']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Poisoning: Metric Drops Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL POISONING: METRIC DROPS FROM BASELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Malicious %':<15} {'TFF Acc Drop':>15} {'Flower Acc Drop':>15} {'TFF F1 Drop':>15} {'Flower F1 Drop':>15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for pct in sorted(tff_model_poison.keys()):\n",
    "    tff_acc_drop = tff_model_poison[pct]['accuracy_drop']\n",
    "    flower_acc_drop = flower_model_poison[pct]['accuracy_drop']\n",
    "    tff_f1_drop = tff_model_poison[pct]['f1_drop']\n",
    "    flower_f1_drop = flower_model_poison[pct]['f1_drop']\n",
    "    print(f\"{pct:<15} {tff_acc_drop:>+15.4f} {flower_acc_drop:>+15.4f} {tff_f1_drop:>+15.4f} {flower_f1_drop:>+15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Poisoning Impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "percentages = sorted(tff_model_poison.keys())\n",
    "x_labels = ['Baseline'] + [p for p in percentages]\n",
    "x = np.arange(len(x_labels))\n",
    "width = 0.35\n",
    "\n",
    "# Accuracy comparison\n",
    "tff_acc = [tff_baseline['accuracy']] + [tff_model_poison[p]['metrics_after']['accuracy'] for p in percentages]\n",
    "flower_acc = [flower_baseline['accuracy']] + [flower_model_poison[p]['metrics_after']['accuracy'] for p in percentages]\n",
    "\n",
    "axes[0].bar(x - width/2, tff_acc, width, label='TFF', color='#2196F3')\n",
    "axes[0].bar(x + width/2, flower_acc, width, label='Flower', color='#4CAF50')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Poisoning: Accuracy Impact')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(x_labels)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# F1 Score comparison\n",
    "tff_f1 = [tff_baseline['f1_score']] + [tff_model_poison[p]['metrics_after']['f1_score'] for p in percentages]\n",
    "flower_f1 = [flower_baseline['f1_score']] + [flower_model_poison[p]['metrics_after']['f1_score'] for p in percentages]\n",
    "\n",
    "axes[1].bar(x - width/2, tff_f1, width, label='TFF', color='#2196F3')\n",
    "axes[1].bar(x + width/2, flower_f1, width, label='Flower', color='#4CAF50')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('Model Poisoning: F1 Score Impact')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(x_labels)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle('Security Analysis: Model Poisoning (Gradient +100)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_poisoning_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Security Analysis: Data Poisoning (Label Flipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data poisoning results\n",
    "tff_data_poison = tff_results['poisoning_analysis']['data_poisoning']\n",
    "flower_data_poison = flower_results['poisoning_analysis']['data_poisoning']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECURITY ANALYSIS: DATA POISONING (Label Flipping)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TFF - Data Poisoning Results\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Malicious %':<15} {'Accuracy':>12} {'F1 Score':>12} {'Precision':>12} {'Recall':>12} {'Loss':>12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Baseline':<15} {tff_baseline['accuracy']:>12.4f} {tff_baseline['f1_score']:>12.4f} {tff_baseline['precision']:>12.4f} {tff_baseline['recall']:>12.4f} {tff_baseline['loss']:>12.4f}\")\n",
    "\n",
    "for pct, data in sorted(tff_data_poison.items()):\n",
    "    m = data['metrics_after']\n",
    "    print(f\"{pct:<15} {m['accuracy']:>12.4f} {m['f1_score']:>12.4f} {m['precision']:>12.4f} {m['recall']:>12.4f} {m['loss']:>12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Flower - Data Poisoning Results\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Malicious %':<15} {'Accuracy':>12} {'F1 Score':>12} {'Precision':>12} {'Recall':>12} {'Loss':>12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Baseline':<15} {flower_baseline['accuracy']:>12.4f} {flower_baseline['f1_score']:>12.4f} {flower_baseline['precision']:>12.4f} {flower_baseline['recall']:>12.4f} {flower_baseline['loss']:>12.4f}\")\n",
    "\n",
    "for pct, data in sorted(flower_data_poison.items()):\n",
    "    m = data['metrics_after']\n",
    "    print(f\"{pct:<15} {m['accuracy']:>12.4f} {m['f1_score']:>12.4f} {m['precision']:>12.4f} {m['recall']:>12.4f} {m['loss']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Poisoning: Metric Drops Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA POISONING: METRIC DROPS FROM BASELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Malicious %':<15} {'TFF Acc Drop':>15} {'Flower Acc Drop':>15} {'TFF F1 Drop':>15} {'Flower F1 Drop':>15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for pct in sorted(tff_data_poison.keys()):\n",
    "    tff_acc_drop = tff_data_poison[pct]['accuracy_drop']\n",
    "    flower_acc_drop = flower_data_poison[pct]['accuracy_drop']\n",
    "    tff_f1_drop = tff_data_poison[pct]['f1_drop']\n",
    "    flower_f1_drop = flower_data_poison[pct]['f1_drop']\n",
    "    print(f\"{pct:<15} {tff_acc_drop:>+15.4f} {flower_acc_drop:>+15.4f} {tff_f1_drop:>+15.4f} {flower_f1_drop:>+15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Data Poisoning Impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "percentages = sorted(tff_data_poison.keys())\n",
    "x_labels = ['Baseline'] + [p for p in percentages]\n",
    "x = np.arange(len(x_labels))\n",
    "width = 0.35\n",
    "\n",
    "# Accuracy comparison\n",
    "tff_acc = [tff_baseline['accuracy']] + [tff_data_poison[p]['metrics_after']['accuracy'] for p in percentages]\n",
    "flower_acc = [flower_baseline['accuracy']] + [flower_data_poison[p]['metrics_after']['accuracy'] for p in percentages]\n",
    "\n",
    "axes[0].bar(x - width/2, tff_acc, width, label='TFF', color='#2196F3')\n",
    "axes[0].bar(x + width/2, flower_acc, width, label='Flower', color='#4CAF50')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Data Poisoning: Accuracy Impact')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(x_labels)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# F1 Score comparison\n",
    "tff_f1 = [tff_baseline['f1_score']] + [tff_data_poison[p]['metrics_after']['f1_score'] for p in percentages]\n",
    "flower_f1 = [flower_baseline['f1_score']] + [flower_data_poison[p]['metrics_after']['f1_score'] for p in percentages]\n",
    "\n",
    "axes[1].bar(x - width/2, tff_f1, width, label='TFF', color='#2196F3')\n",
    "axes[1].bar(x + width/2, flower_f1, width, label='Flower', color='#4CAF50')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('Data Poisoning: F1 Score Impact')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(x_labels)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle('Security Analysis: Data Poisoning (Label Flipping)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_poisoning_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Security Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average robustness scores\n",
    "def calculate_robustness(poison_results, baseline):\n",
    "    \"\"\"Calculate robustness as 1 - average metric drop.\"\"\"\n",
    "    acc_drops = []\n",
    "    f1_drops = []\n",
    "    for pct, data in poison_results.items():\n",
    "        acc_drops.append(data['accuracy_drop'])\n",
    "        f1_drops.append(data['f1_drop'])\n",
    "    avg_drop = (np.mean(acc_drops) + np.mean(f1_drops)) / 2\n",
    "    return max(0, 1 - avg_drop)\n",
    "\n",
    "# TFF robustness\n",
    "tff_model_robust = calculate_robustness(tff_model_poison, tff_baseline)\n",
    "tff_data_robust = calculate_robustness(tff_data_poison, tff_baseline)\n",
    "tff_overall_robust = (tff_model_robust + tff_data_robust) / 2\n",
    "\n",
    "# Flower robustness\n",
    "flower_model_robust = calculate_robustness(flower_model_poison, flower_baseline)\n",
    "flower_data_robust = calculate_robustness(flower_data_poison, flower_baseline)\n",
    "flower_overall_robust = (flower_model_robust + flower_data_robust) / 2\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECURITY ROBUSTNESS SCORES (Higher is Better)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<30} {'TFF':>15} {'Flower':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Model Poisoning Robustness':<30} {tff_model_robust:>15.4f} {flower_model_robust:>15.4f}\")\n",
    "print(f\"{'Data Poisoning Robustness':<30} {tff_data_robust:>15.4f} {flower_data_robust:>15.4f}\")\n",
    "print(f\"{'Overall Robustness':<30} {tff_overall_robust:>15.4f} {flower_overall_robust:>15.4f}\")\n",
    "\n",
    "more_robust = \"TFF\" if tff_overall_robust > flower_overall_robust else \"Flower\"\n",
    "print(f\"\\n→ {more_robust} is more robust against poisoning attacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization: All metrics at 20% malicious\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Get 20% results (most common test case)\n",
    "pct_key = '20pct'\n",
    "\n",
    "metrics = ['accuracy', 'f1_score', 'precision', 'recall']\n",
    "titles = ['Accuracy', 'F1 Score', 'Precision', 'Recall']\n",
    "\n",
    "for ax, metric, title in zip(axes.flat, metrics, titles):\n",
    "    # Data for grouped bar chart\n",
    "    scenarios = ['Baseline', 'Model\\nPoisoning', 'Data\\nPoisoning']\n",
    "    x = np.arange(len(scenarios))\n",
    "    width = 0.35\n",
    "    \n",
    "    tff_vals = [\n",
    "        tff_baseline[metric],\n",
    "        tff_model_poison[pct_key]['metrics_after'][metric],\n",
    "        tff_data_poison[pct_key]['metrics_after'][metric]\n",
    "    ]\n",
    "    flower_vals = [\n",
    "        flower_baseline[metric],\n",
    "        flower_model_poison[pct_key]['metrics_after'][metric],\n",
    "        flower_data_poison[pct_key]['metrics_after'][metric]\n",
    "    ]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, tff_vals, width, label='TFF', color='#2196F3')\n",
    "    bars2 = ax.bar(x + width/2, flower_vals, width, label='Flower', color='#4CAF50')\n",
    "    \n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(f'{title} at 20% Malicious Clients')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(scenarios)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{bar.get_height():.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    for bar in bars2:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{bar.get_height():.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Security Analysis: All Metrics at 20% Malicious Clients', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('security_comprehensive.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TECHNICAL FACTORS COMPARISON: FINAL REPORT\")\n",
    "print(\"TensorFlow Federated vs Flower for Bug Prediction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"1. COMMUNICATION COST\")\n",
    "print(\"-\"*80)\n",
    "print(f\"   TFF Total:    {tff_comm['total_bytes']/1e6:.2f} MB\")\n",
    "print(f\"   Flower Total: {flower_comm['total_bytes']/1e6:.2f} MB\")\n",
    "comm_winner = \"Flower\" if flower_comm['total_bytes'] < tff_comm['total_bytes'] else \"TFF\"\n",
    "print(f\"   Winner: {comm_winner}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"2. SECURITY: MODEL POISONING (Gradient +100)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"   TFF Robustness Score:    {tff_model_robust:.4f}\")\n",
    "print(f\"   Flower Robustness Score: {flower_model_robust:.4f}\")\n",
    "model_winner = \"TFF\" if tff_model_robust > flower_model_robust else \"Flower\"\n",
    "print(f\"   Winner: {model_winner}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"3. SECURITY: DATA POISONING (Label Flipping)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"   TFF Robustness Score:    {tff_data_robust:.4f}\")\n",
    "print(f\"   Flower Robustness Score: {flower_data_robust:.4f}\")\n",
    "data_winner = \"TFF\" if tff_data_robust > flower_data_robust else \"Flower\"\n",
    "print(f\"   Winner: {data_winner}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4. OVERALL SECURITY ROBUSTNESS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"   TFF Overall:    {tff_overall_robust:.4f}\")\n",
    "print(f\"   Flower Overall: {flower_overall_robust:.4f}\")\n",
    "overall_winner = \"TFF\" if tff_overall_robust > flower_overall_robust else \"Flower\"\n",
    "print(f\"   Winner: {overall_winner}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n   Communication Cost Winner:     {comm_winner}\")\n",
    "print(f\"   Model Poisoning Robustness:    {model_winner}\")\n",
    "print(f\"   Data Poisoning Robustness:     {data_winner}\")\n",
    "print(f\"   Overall Security Winner:       {overall_winner}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Key Findings:\")\n",
    "print(\"-\"*80)\n",
    "print(\"   • Model poisoning (+100 to gradients) significantly impacts both frameworks\")\n",
    "print(\"   • Data poisoning (label flipping) has severe effects on training quality\")\n",
    "print(\"   • Both frameworks need Byzantine-robust aggregation for production use\")\n",
    "print(\"   • Consider gradient clipping and anomaly detection as defenses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comprehensive results\n",
    "comparison_results = {\n",
    "    'technical_factors': {\n",
    "        'communication_cost': {\n",
    "            'tff': tff_comm,\n",
    "            'flower': flower_comm,\n",
    "            'winner': comm_winner\n",
    "        },\n",
    "        'security': {\n",
    "            'model_poisoning': {\n",
    "                'attack_description': 'Add +100 to gradients from malicious clients',\n",
    "                'tff_results': tff_model_poison,\n",
    "                'flower_results': flower_model_poison,\n",
    "                'tff_robustness': tff_model_robust,\n",
    "                'flower_robustness': flower_model_robust,\n",
    "                'winner': model_winner\n",
    "            },\n",
    "            'data_poisoning': {\n",
    "                'attack_description': 'Flip labels (buggy <-> non-buggy) for malicious clients',\n",
    "                'tff_results': tff_data_poison,\n",
    "                'flower_results': flower_data_poison,\n",
    "                'tff_robustness': tff_data_robust,\n",
    "                'flower_robustness': flower_data_robust,\n",
    "                'winner': data_winner\n",
    "            },\n",
    "            'overall_robustness': {\n",
    "                'tff': tff_overall_robust,\n",
    "                'flower': flower_overall_robust,\n",
    "                'winner': overall_winner\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'baseline_comparison': {\n",
    "        'tff': tff_baseline,\n",
    "        'flower': flower_baseline\n",
    "    },\n",
    "    'summary': {\n",
    "        'communication_winner': comm_winner,\n",
    "        'model_poisoning_winner': model_winner,\n",
    "        'data_poisoning_winner': data_winner,\n",
    "        'overall_security_winner': overall_winner\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('comparison_results.json', 'w') as f:\n",
    "    json.dump(comparison_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to comparison_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all results\n",
    "from google.colab import files\n",
    "\n",
    "files.download('comparison_results.json')\n",
    "files.download('communication_comparison.png')\n",
    "files.download('model_poisoning_comparison.png')\n",
    "files.download('data_poisoning_comparison.png')\n",
    "files.download('security_comprehensive.png')\n",
    "\n",
    "print(\"\\nAll files downloaded:\")\n",
    "print(\"  - comparison_results.json\")\n",
    "print(\"  - communication_comparison.png\")\n",
    "print(\"  - model_poisoning_comparison.png\")\n",
    "print(\"  - data_poisoning_comparison.png\")\n",
    "print(\"  - security_comprehensive.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook compared TFF and Flower on two technical factors:\n",
    "\n",
    "### 1. Communication Cost\n",
    "- Measured total bytes sent/received during training\n",
    "- Compared per-round communication overhead\n",
    "\n",
    "### 2. Security (Poisoning Attacks)\n",
    "- **Model Poisoning**: Added +100 to gradients\n",
    "- **Data Poisoning**: Flipped labels (buggy ↔ non-buggy)\n",
    "- Measured impact on: Accuracy, F1, Precision, Recall, Loss\n",
    "- Calculated robustness scores for each framework\n",
    "\n",
    "### Output Files\n",
    "- `comparison_results.json`: Complete comparison data\n",
    "- PNG visualizations for all analyses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
