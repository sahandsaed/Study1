{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow FL for Bug Prediction - Improved Version\n",
    "\n",
    "## Key Improvements:\n",
    "1. **Better Model**: Proper feature normalization, optimized architecture\n",
    "2. **Effective Poisoning Attacks**:\n",
    "   - Model Poisoning: Sign-flip attack (negate weights)\n",
    "   - Data Poisoning: Strategic label flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow scikit-learn -q\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"Upload dataset_pairs_1_.json\")\n",
    "uploaded = files.upload()\n",
    "dataset_path = list(uploaded.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_features(code: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract discriminative features that differ between buggy and fixed code.\n",
    "    \"\"\"\n",
    "    lines = code.split('\\n')\n",
    "    non_empty_lines = [l for l in lines if l.strip()]\n",
    "    \n",
    "    # Basic metrics\n",
    "    total_chars = len(code)\n",
    "    num_lines = len(lines)\n",
    "    num_non_empty = len(non_empty_lines)\n",
    "    avg_line_len = np.mean([len(l) for l in lines]) if lines else 0\n",
    "    max_line_len = max([len(l) for l in lines]) if lines else 0\n",
    "    \n",
    "    # Code structure\n",
    "    num_functions = code.count('def ')\n",
    "    num_classes = code.count('class ')\n",
    "    num_imports = code.count('import ')\n",
    "    num_returns = code.count('return ')\n",
    "    \n",
    "    # Control flow\n",
    "    num_if = code.count('if ') + code.count('elif ')\n",
    "    num_else = code.count('else:')\n",
    "    num_for = code.count('for ')\n",
    "    num_while = code.count('while ')\n",
    "    num_try = code.count('try:')\n",
    "    num_except = code.count('except')\n",
    "    num_finally = code.count('finally:')\n",
    "    num_with = code.count('with ')\n",
    "    \n",
    "    # Operators (bug-prone)\n",
    "    num_eq = code.count(' == ')\n",
    "    num_neq = code.count(' != ')\n",
    "    num_lt = code.count(' < ') + code.count(' <= ')\n",
    "    num_gt = code.count(' > ') + code.count(' >= ')\n",
    "    num_and = code.count(' and ')\n",
    "    num_or = code.count(' or ')\n",
    "    num_not = code.count(' not ') + code.count('not(')\n",
    "    \n",
    "    # Assignment and arithmetic\n",
    "    num_assign = code.count(' = ') - num_eq - num_neq\n",
    "    num_plus_eq = code.count(' += ')\n",
    "    num_minus_eq = code.count(' -= ')\n",
    "    num_arithmetic = code.count(' + ') + code.count(' - ') + code.count(' * ') + code.count(' / ')\n",
    "    num_modulo = code.count(' % ')\n",
    "    \n",
    "    # Data structures\n",
    "    num_list_access = code.count('[') \n",
    "    num_dict_access = code.count('{')\n",
    "    num_tuple = code.count('(')\n",
    "    \n",
    "    # Common operations\n",
    "    num_append = code.count('.append(')\n",
    "    num_extend = code.count('.extend(')\n",
    "    num_pop = code.count('.pop(')\n",
    "    num_remove = code.count('.remove(')\n",
    "    num_len = code.count('len(')\n",
    "    num_range = code.count('range(')\n",
    "    num_enumerate = code.count('enumerate(')\n",
    "    num_zip = code.count('zip(')\n",
    "    \n",
    "    # String operations\n",
    "    num_str_format = code.count('.format(') + code.count('f\"') + code.count(\"f'\")\n",
    "    num_split = code.count('.split(')\n",
    "    num_join = code.count('.join(')\n",
    "    num_strip = code.count('.strip(') + code.count('.rstrip(') + code.count('.lstrip(')\n",
    "    \n",
    "    # Type operations\n",
    "    num_int = code.count('int(')\n",
    "    num_float = code.count('float(')\n",
    "    num_str = code.count('str(')\n",
    "    num_list = code.count('list(')\n",
    "    num_dict = code.count('dict(')\n",
    "    \n",
    "    # Error-prone patterns\n",
    "    num_none = code.count('None')\n",
    "    num_true = code.count('True')\n",
    "    num_false = code.count('False')\n",
    "    num_is = code.count(' is ')\n",
    "    num_in = code.count(' in ')\n",
    "    \n",
    "    # Indentation (complexity)\n",
    "    indent_levels = [len(l) - len(l.lstrip()) for l in non_empty_lines] if non_empty_lines else [0]\n",
    "    max_indent = max(indent_levels)\n",
    "    avg_indent = np.mean(indent_levels)\n",
    "    \n",
    "    # Comments\n",
    "    num_comments = code.count('#')\n",
    "    num_docstrings = code.count('\"\"\"') + code.count(\"'''\")\n",
    "    \n",
    "    features = np.array([\n",
    "        total_chars, num_lines, num_non_empty, avg_line_len, max_line_len,\n",
    "        num_functions, num_classes, num_imports, num_returns,\n",
    "        num_if, num_else, num_for, num_while, num_try, num_except, num_finally, num_with,\n",
    "        num_eq, num_neq, num_lt, num_gt, num_and, num_or, num_not,\n",
    "        num_assign, num_plus_eq, num_minus_eq, num_arithmetic, num_modulo,\n",
    "        num_list_access, num_dict_access, num_tuple,\n",
    "        num_append, num_extend, num_pop, num_remove, num_len, num_range, num_enumerate, num_zip,\n",
    "        num_str_format, num_split, num_join, num_strip,\n",
    "        num_int, num_float, num_str, num_list, num_dict,\n",
    "        num_none, num_true, num_false, num_is, num_in,\n",
    "        max_indent, avg_indent, num_comments, num_docstrings\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(f\"Number of features: {len(extract_code_features('test'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "with open(dataset_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} code pairs\")\n",
    "\n",
    "# Extract features\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "for pair in data:\n",
    "    # Buggy code -> label 1\n",
    "    features_list.append(extract_code_features(pair['buggy']))\n",
    "    labels_list.append(1)\n",
    "    \n",
    "    # Fixed code -> label 0  \n",
    "    features_list.append(extract_code_features(pair['fixed']))\n",
    "    labels_list.append(0)\n",
    "\n",
    "X = np.array(features_list)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Class distribution: Buggy={sum(y)}, Fixed={len(y)-sum(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features (CRITICAL for good performance)\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Normalized feature range: [{X_normalized.min():.2f}, {X_normalized.max():.2f}]\")\n",
    "print(f\"Mean: {X_normalized.mean():.4f}, Std: {X_normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    \"\"\"\n",
    "    Create a well-tuned model for bug prediction.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # First hidden layer\n",
    "        layers.Dense(128, kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Second hidden layer\n",
    "        layers.Dense(64, kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Third hidden layer\n",
    "        layers.Dense(32, kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Output\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test\n",
    "test_model = create_model(X_normalized.shape[1])\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model can learn (centralized training test)\n",
    "print(\"Testing centralized training to verify model quality...\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "test_model = create_model(X_normalized.shape[1])\n",
    "test_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = test_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = (test_model.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "print(f\"\\n=== Centralized Performance ===\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Metrics:\n",
    "    accuracy: float = 0.0\n",
    "    precision: float = 0.0\n",
    "    recall: float = 0.0\n",
    "    f1_score: float = 0.0\n",
    "    loss: float = 0.0\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class ExpResult:\n",
    "    name: str\n",
    "    metrics: Metrics\n",
    "    time: float = 0.0\n",
    "    comm_bytes: float = 0.0\n",
    "    history: List = field(default_factory=list)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'experiment_name': self.name,\n",
    "            'metrics': self.metrics.to_dict(),\n",
    "            'training_time': self.time,\n",
    "            'communication_bytes': self.comm_bytes,\n",
    "            'round_metrics': self.history\n",
    "        }\n",
    "\n",
    "def calc_metrics(y_true, y_pred, loss=0.0):\n",
    "    return Metrics(\n",
    "        accuracy=accuracy_score(y_true, y_pred),\n",
    "        precision=precision_score(y_true, y_pred, zero_division=0),\n",
    "        recall=recall_score(y_true, y_pred, zero_division=0),\n",
    "        f1_score=f1_score(y_true, y_pred, zero_division=0),\n",
    "        loss=loss\n",
    "    )\n",
    "\n",
    "def model_size(model):\n",
    "    return sum(w.nbytes for w in model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Poisoning Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attacks:\n",
    "    \"\"\"\n",
    "    Realistic poisoning attacks:\n",
    "    \n",
    "    1. Model Poisoning (Sign-Flip): \n",
    "       - Multiply weights by negative factor\n",
    "       - Pushes model away from optimal\n",
    "    \n",
    "    2. Data Poisoning (Label Flip):\n",
    "       - Flip labels on malicious clients\n",
    "       - Model learns wrong patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def model_poison(weights, scale=-2.0):\n",
    "        \"\"\"\n",
    "        Sign-flip attack with amplification.\n",
    "        scale=-2.0 means flip sign and double magnitude.\n",
    "        \"\"\"\n",
    "        return [w * scale for w in weights]\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_poison(labels):\n",
    "        \"\"\"Flip all labels: 0->1, 1->0\"\"\"\n",
    "        return 1.0 - labels\n",
    "\n",
    "print(\"Attacks defined:\")\n",
    "print(\"  - Model Poisoning: Sign-flip with 2x amplification\")\n",
    "print(\"  - Data Poisoning: Label flipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, cid, features, labels, input_dim, malicious=False, attack='none'):\n",
    "        self.cid = cid\n",
    "        self.malicious = malicious\n",
    "        self.attack = attack\n",
    "        \n",
    "        # Data\n",
    "        self.X = features.copy()\n",
    "        self.y = Attacks.data_poison(labels.copy()) if (malicious and attack == 'data') else labels.copy()\n",
    "        self.n_samples = len(self.y)\n",
    "        \n",
    "        # Model\n",
    "        self.model = create_model(input_dim)\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def set_weights(self, w):\n",
    "        self.model.set_weights(w)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        w = self.model.get_weights()\n",
    "        if self.malicious and self.attack == 'model':\n",
    "            w = Attacks.model_poison(w)\n",
    "        return w\n",
    "    \n",
    "    def train(self, epochs=5):\n",
    "        h = self.model.fit(self.X, self.y, epochs=epochs, batch_size=16, verbose=0)\n",
    "        return h.history['loss'][-1]\n",
    "\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, input_dim):\n",
    "        self.model = create_model(input_dim)\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def aggregate(self, client_weights, client_samples):\n",
    "        \"\"\"FedAvg\"\"\"\n",
    "        total = sum(client_samples)\n",
    "        agg = [np.zeros_like(w) for w in client_weights[0]]\n",
    "        for w, n in zip(client_weights, client_samples):\n",
    "            for i in range(len(w)):\n",
    "                agg[i] += w[i] * (n / total)\n",
    "        self.model.set_weights(agg)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        pred = (self.model.predict(X, verbose=0) > 0.5).astype(int).flatten()\n",
    "        loss = self.model.evaluate(X, y, verbose=0)[0]\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    client_data,\n",
    "    input_dim,\n",
    "    num_rounds=25,\n",
    "    local_epochs=5,\n",
    "    malicious_ids=None,\n",
    "    attack='none',\n",
    "    exp_name='baseline'\n",
    "):\n",
    "    if malicious_ids is None:\n",
    "        malicious_ids = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Experiment: {exp_name}\")\n",
    "    print(f\"Attack: {attack}, Malicious: {malicious_ids}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    history = []\n",
    "    \n",
    "    # Server\n",
    "    server = Server(input_dim)\n",
    "    msize = model_size(server.model)\n",
    "    total_comm = 0\n",
    "    \n",
    "    # Clients\n",
    "    clients = []\n",
    "    for i, d in enumerate(client_data):\n",
    "        mal = i in malicious_ids\n",
    "        c = Client(i, d['X'], d['y'], input_dim, malicious=mal, attack=attack)\n",
    "        clients.append(c)\n",
    "        if mal:\n",
    "            print(f\"  Client {i}: MALICIOUS ({attack})\")\n",
    "    \n",
    "    # Global test data\n",
    "    X_all = np.concatenate([d['X'] for d in client_data])\n",
    "    y_all = np.concatenate([d['y'] for d in client_data])\n",
    "    \n",
    "    # FL Loop\n",
    "    for r in range(1, num_rounds + 1):\n",
    "        # Distribute\n",
    "        gw = server.get_weights()\n",
    "        for c in clients:\n",
    "            c.set_weights(copy.deepcopy(gw))\n",
    "        \n",
    "        # Train\n",
    "        cw_list = []\n",
    "        cs_list = []\n",
    "        for c in clients:\n",
    "            c.train(epochs=local_epochs)\n",
    "            cw_list.append(c.get_weights())\n",
    "            cs_list.append(c.n_samples)\n",
    "        \n",
    "        # Aggregate\n",
    "        server.aggregate(cw_list, cs_list)\n",
    "        \n",
    "        # Comm\n",
    "        total_comm += msize * len(clients) * 2\n",
    "        \n",
    "        # Eval\n",
    "        pred, loss = server.evaluate(X_all, y_all)\n",
    "        acc = accuracy_score(y_all, pred)\n",
    "        f1 = f1_score(y_all, pred)\n",
    "        \n",
    "        history.append({'round': r, 'accuracy': acc, 'f1_score': f1, 'loss': loss})\n",
    "        \n",
    "        if r % 5 == 0 or r == num_rounds:\n",
    "            print(f\"  Round {r:2d}/{num_rounds} - Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    # Final\n",
    "    pred, loss = server.evaluate(X_all, y_all)\n",
    "    final = calc_metrics(y_all, pred, loss)\n",
    "    \n",
    "    print(f\"\\nFinal: Acc={final.accuracy:.4f}, F1={final.f1_score:.4f}, Prec={final.precision:.4f}, Rec={final.recall:.4f}\")\n",
    "    \n",
    "    return ExpResult(name=exp_name, metrics=final, time=time.time()-t0, comm_bytes=total_comm, history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "NUM_CLIENTS = 10\n",
    "NUM_ROUNDS = 25\n",
    "LOCAL_EPOCHS = 5\n",
    "MALICIOUS_FRACTIONS = [0.1, 0.2, 0.3]\n",
    "\n",
    "# Partition data IID\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(y))\n",
    "X_shuffled = X_normalized[indices]\n",
    "y_shuffled = y[indices]\n",
    "\n",
    "splits = np.array_split(np.arange(len(y)), NUM_CLIENTS)\n",
    "client_data = [{'X': X_shuffled[s], 'y': y_shuffled[s]} for s in splits]\n",
    "\n",
    "input_dim = X_normalized.shape[1]\n",
    "\n",
    "print(f\"Clients: {NUM_CLIENTS}\")\n",
    "print(f\"Samples per client: ~{len(y)//NUM_CLIENTS}\")\n",
    "print(f\"Features: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# BASELINE\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "results['baseline'] = run_experiment(\n",
    "    client_data, input_dim, NUM_ROUNDS, LOCAL_EPOCHS,\n",
    "    malicious_ids=[], attack='none', exp_name='baseline'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL POISONING\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL POISONING (Sign-Flip Attack)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for frac in MALICIOUS_FRACTIONS:\n",
    "    n_mal = int(NUM_CLIENTS * frac)\n",
    "    mal_ids = list(range(n_mal))\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "    name = f'model_poison_{int(frac*100)}pct'\n",
    "    results[name] = run_experiment(\n",
    "        client_data, input_dim, NUM_ROUNDS, LOCAL_EPOCHS,\n",
    "        malicious_ids=mal_ids, attack='model', exp_name=name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA POISONING\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA POISONING (Label Flipping)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for frac in MALICIOUS_FRACTIONS:\n",
    "    n_mal = int(NUM_CLIENTS * frac)\n",
    "    mal_ids = list(range(n_mal))\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "    name = f'data_poison_{int(frac*100)}pct'\n",
    "    results[name] = run_experiment(\n",
    "        client_data, input_dim, NUM_ROUNDS, LOCAL_EPOCHS,\n",
    "        malicious_ids=mal_ids, attack='data', exp_name=name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n{'Experiment':<25} {'Accuracy':>12} {'F1 Score':>12} {'Precision':>12} {'Recall':>12} {'Loss':>12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for name, r in results.items():\n",
    "    m = r.metrics\n",
    "    print(f\"{name:<25} {m.accuracy:>12.4f} {m.f1_score:>12.4f} {m.precision:>12.4f} {m.recall:>12.4f} {m.loss:>12.4f}\")\n",
    "\n",
    "base = results['baseline'].metrics\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"DROPS FROM BASELINE\")\n",
    "print(\"-\"*100)\n",
    "print(f\"\\n{'Experiment':<25} {'Acc Drop':>12} {'F1 Drop':>12} {'Prec Drop':>12} {'Rec Drop':>12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for name, r in results.items():\n",
    "    if name == 'baseline':\n",
    "        continue\n",
    "    m = r.metrics\n",
    "    print(f\"{name:<25} {base.accuracy-m.accuracy:>+12.4f} {base.f1_score-m.f1_score:>+12.4f} {base.precision-m.precision:>+12.4f} {base.recall-m.recall:>+12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "names = list(results.keys())\n",
    "x = np.arange(len(names))\n",
    "colors = ['green' if 'baseline' in n else 'orange' if 'model' in n else 'red' for n in names]\n",
    "\n",
    "for ax, (metric, title) in zip(axes.flat, [\n",
    "    ('accuracy', 'Accuracy'), ('f1_score', 'F1 Score'),\n",
    "    ('precision', 'Precision'), ('recall', 'Recall')\n",
    "]):\n",
    "    vals = [getattr(results[n].metrics, metric) for n in names]\n",
    "    bars = ax.bar(x, vals, color=colors)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([n.replace('_', '\\n') for n in names], rotation=45, ha='right', fontsize=8)\n",
    "    ax.axhline(y=getattr(base, metric), color='green', linestyle='--', alpha=0.7)\n",
    "    ax.set_ylim(0, 1)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{v:.2f}', ha='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('TensorFlow FL: Poisoning Attack Impact', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tff_poisoning_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, r in results.items():\n",
    "    rounds = [h['round'] for h in r.history]\n",
    "    accs = [h['accuracy'] for h in r.history]\n",
    "    f1s = [h['f1_score'] for h in r.history]\n",
    "    \n",
    "    ls = '-' if 'baseline' in name else '--' if 'model' in name else ':'\n",
    "    lw = 2.5 if 'baseline' in name else 1.5\n",
    "    axes[0].plot(rounds, accs, label=name, linestyle=ls, linewidth=lw)\n",
    "    axes[1].plot(rounds, f1s, label=name, linestyle=ls, linewidth=lw)\n",
    "\n",
    "axes[0].set_xlabel('Round')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy Over Rounds')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Round')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('F1 Score Over Rounds')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Progression', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tff_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = {\n",
    "    'framework': 'TensorFlow Federated',\n",
    "    'config': {\n",
    "        'num_clients': NUM_CLIENTS,\n",
    "        'num_rounds': NUM_ROUNDS,\n",
    "        'local_epochs': LOCAL_EPOCHS,\n",
    "        'malicious_fractions': MALICIOUS_FRACTIONS\n",
    "    },\n",
    "    'experiments': {n: r.to_dict() for n, r in results.items()},\n",
    "    'baseline_metrics': results['baseline'].metrics.to_dict(),\n",
    "    'communication': {\n",
    "        'total_bytes': results['baseline'].comm_bytes,\n",
    "        'per_round_bytes': results['baseline'].comm_bytes / NUM_ROUNDS\n",
    "    },\n",
    "    'poisoning_analysis': {\n",
    "        'model_poisoning': {},\n",
    "        'data_poisoning': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, r in results.items():\n",
    "    if 'model_poison' in name:\n",
    "        pct = name.split('_')[-1]\n",
    "        export['poisoning_analysis']['model_poisoning'][pct] = {\n",
    "            'metrics_after': r.metrics.to_dict(),\n",
    "            'accuracy_drop': base.accuracy - r.metrics.accuracy,\n",
    "            'f1_drop': base.f1_score - r.metrics.f1_score,\n",
    "            'precision_drop': base.precision - r.metrics.precision,\n",
    "            'recall_drop': base.recall - r.metrics.recall,\n",
    "            'loss_increase': r.metrics.loss - base.loss\n",
    "        }\n",
    "    elif 'data_poison' in name:\n",
    "        pct = name.split('_')[-1]\n",
    "        export['poisoning_analysis']['data_poisoning'][pct] = {\n",
    "            'metrics_after': r.metrics.to_dict(),\n",
    "            'accuracy_drop': base.accuracy - r.metrics.accuracy,\n",
    "            'f1_drop': base.f1_score - r.metrics.f1_score,\n",
    "            'precision_drop': base.precision - r.metrics.precision,\n",
    "            'recall_drop': base.recall - r.metrics.recall,\n",
    "            'loss_increase': r.metrics.loss - base.loss\n",
    "        }\n",
    "\n",
    "with open('tff_results.json', 'w') as f:\n",
    "    json.dump(export, f, indent=2)\n",
    "\n",
    "print(\"Saved tff_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('tff_results.json')\n",
    "files.download('tff_poisoning_results.png')\n",
    "files.download('tff_training_curves.png')\n",
    "print(\"Downloads complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
